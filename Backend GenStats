from fastapi import FastAPI, UploadFile, File, HTTPException
import pandas as pd
import numpy as np
import io
import uvicorn
from pydantic import BaseModel
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from statsmodels.stats.weightstats import ttest_ind
import matplotlib.pyplot as plt
import seaborn as sns
import os
import uuid
import joblib
import json
from scipy import stats
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, r2_score, mean_squared_error
import torch
from transformers import pipeline

app = FastAPI()

# Directory to save generated plots and models
PLOT_DIR = "generated_plots"
MODEL_DIR = "saved_models"
os.makedirs(PLOT_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)

# Data storage (for demo purposes, replace with a database in production)
uploaded_data = {}
llm_pipeline = pipeline("text2text-generation", model="google/flan-t5-large")

class PreprocessRequest(BaseModel):
    method: str  # "standardize", "normalize", "impute"

class ModelTrainingRequest(BaseModel):
    target_column: str
    model_type: str  # "linear_regression", "random_forest"

class TTestRequest(BaseModel):
    col1: str
    col2: str

class LLMRequest(BaseModel):
    query: str

@app.post("/upload")
async def upload_dataset(file: UploadFile = File(...)):
    """Uploads a CSV file and stores it temporarily."""
    try:
        contents = await file.read()
        df = pd.read_csv(io.StringIO(contents.decode("utf-8")))
        file_id = str(uuid.uuid4())
        uploaded_data[file_id] = df
        return {"message": "File uploaded successfully", "file_id": file_id, "columns": df.columns.tolist()}
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Error processing file: {str(e)}")

@app.get("/summary/{file_id}")
def get_data_summary(file_id: str):
    """Generates a summary of the dataset."""
    if file_id not in uploaded_data:
        raise HTTPException(status_code=404, detail="File not found")
    df = uploaded_data[file_id]
    return df.describe().to_dict()

@app.post("/preprocess/{file_id}")
def preprocess_data(file_id: str, request: PreprocessRequest):
    """Applies preprocessing methods like normalization, standardization, and imputation."""
    if file_id not in uploaded_data:
        raise HTTPException(status_code=404, detail="File not found")
    df = uploaded_data[file_id]
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if request.method == "standardize":
        scaler = StandardScaler()
        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])
    elif request.method == "normalize":
        scaler = MinMaxScaler()
        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])
    elif request.method == "impute":
        imputer = SimpleImputer(strategy='mean')
        df[numeric_cols] = imputer.fit_transform(df[numeric_cols])
    else:
        raise HTTPException(status_code=400, detail="Invalid preprocessing method")
    return {"message": f"Data {request.method} applied", "processed_data": df.head().to_dict()}

@app.post("/train_model/{file_id}")
def train_model(file_id: str, request: ModelTrainingRequest):
    """Trains a machine learning model on the dataset."""
    if file_id not in uploaded_data:
        raise HTTPException(status_code=404, detail="File not found")
    df = uploaded_data[file_id]
    if request.target_column not in df.columns:
        raise HTTPException(status_code=400, detail="Invalid target column")
    X = df.drop(columns=[request.target_column])
    y = df[request.target_column]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    if request.model_type == "linear_regression":
        model = LinearRegression()
    elif request.model_type == "random_forest":
        model = RandomForestClassifier()
    else:
        raise HTTPException(status_code=400, detail="Invalid model type")
    
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    metrics = {"r2_score": r2_score(y_test, predictions), "mse": mean_squared_error(y_test, predictions)}
    model_path = f"{MODEL_DIR}/{file_id}_{request.model_type}.joblib"
    joblib.dump(model, model_path)
    return {"message": "Model trained successfully", "metrics": metrics, "model_path": model_path}

@app.post("/generate_insights")
def generate_insights(request: LLMRequest):
    """Leverages LLM to generate AI-powered insights on statistical queries."""
    response = llm_pipeline(request.query, max_length=256, truncation=True)
    return {"response": response[0]['generated_text']}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)